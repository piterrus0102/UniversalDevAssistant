package rag

import config.ProjectConfig
import mu.KotlinLogging
import kotlinx.serialization.Serializable
import kotlinx.serialization.encodeToString
import kotlinx.serialization.json.Json
import java.io.File
import java.nio.file.Files
import java.nio.file.Path
import java.nio.file.Paths
import kotlin.io.path.extension
import kotlin.io.path.isRegularFile
import kotlin.io.path.readText
import kotlin.streams.toList

private val logger = KotlinLogging.logger {}

/**
 * –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫
 */
@Serializable
data class RAGIndex(
    val projectName: String,
    val projectPath: String,
    val documents: List<Document>,
    val chunks: List<DocumentChunk>,  // –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫–∏
    val embeddings: List<List<Double>>,
    val timestamp: Long,
    val vectorizationEnabled: Boolean
)

class RAGService(
    private val config: ProjectConfig,
    private val ollamaClient: OllamaClient? = null,
    private val reranker: Reranker? = null  // –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π reranker –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
) {
    private val documents = mutableListOf<Document>()
    private val chunks = mutableListOf<DocumentChunk>()  // –ß–∞–Ω–∫–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    private val documentEmbeddings = mutableListOf<List<Double>>()
    private val projectPath = Paths.get(config.project.path)
    private val vectorizationEnabled = config.vectorization?.enabled == true && ollamaClient != null
    
    // –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–µ—à–∞ –∏–Ω–¥–µ–∫—Å–∞
    private val indexCacheFile = File("src/main/kotlin/rag/index.json")
    
    private val json = Json {
        prettyPrint = true
        ignoreUnknownKeys = true
    }
    
    /**
     * –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏–∑ –∫–µ—à–∞ –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
     */
    fun loadIndexIfExists(): Boolean {
        if (!indexCacheFile.exists()) {
            logger.info { "üìÇ –§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: ${indexCacheFile.path}" }
            return false
        }
        
        return try {
            logger.info { "üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –∫–µ—à–∞: ${indexCacheFile.path}" }
            val indexJson = indexCacheFile.readText()
            val index = json.decodeFromString<RAGIndex>(indexJson)
            
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∏–Ω–¥–µ–∫—Å –¥–ª—è –Ω—É–∂–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
            if (index.projectPath != config.project.path) {
                logger.warn { "‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å –¥–ª—è –¥—Ä—É–≥–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ (${index.projectPath})" }
                return false
            }
            
            documents.clear()
            documents.addAll(index.documents)
            
            chunks.clear()
            chunks.addAll(index.chunks)
            
            documentEmbeddings.clear()
            documentEmbeddings.addAll(index.embeddings)
            
            logger.info { "‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: ${documents.size} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, ${chunks.size} —á–∞–Ω–∫–æ–≤, ${documentEmbeddings.size} –≤–µ–∫—Ç–æ—Ä–æ–≤" }
            logger.info { "üìÖ –í—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: ${java.time.Instant.ofEpochMilli(index.timestamp)}" }
            logger.info { "üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: ${documents.sumOf { it.size } / 1024} KB" }
            
            true
        } catch (e: Exception) {
            logger.error(e) { "‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è" }
            false
        }
    }
    
    /**
     * –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ –∫–µ—à
     */
    private fun saveIndex() {
        try {
            logger.info { "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –∫–µ—à: ${indexCacheFile.path}" }
            
            // –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            indexCacheFile.parentFile?.mkdirs()
            
            val index = RAGIndex(
                projectName = config.project.name,
                projectPath = config.project.path,
                documents = documents.toList(),
                chunks = chunks.toList(),
                embeddings = documentEmbeddings.toList(),
                timestamp = System.currentTimeMillis(),
                vectorizationEnabled = vectorizationEnabled
            )
            
            val indexJson = json.encodeToString(index)
            indexCacheFile.writeText(indexJson)
            
            logger.info { "‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω (${indexCacheFile.length() / 1024} KB)" }
        } catch (e: Exception) {
            logger.error(e) { "‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞" }
        }
    }
    
    /**
     * –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ config.yaml
     */
    fun indexDocuments() {
        logger.info { "üìñ –ù–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞: ${config.project.name}" }
        logger.info { "üìç –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É: ${config.project.path}" }
        
        documents.clear()
        documentEmbeddings.clear()
        
        config.project.docs.forEach { pattern ->
            logger.debug { "–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: $pattern" }
            val files = findFilesByPattern(pattern)
            
            files.forEach { file ->
                try {
                    val content = file.readText()
                    val doc = Document(
                        path = projectPath.relativize(file).toString(),
                        content = content,
                        lines = content.lines().size,
                        size = Files.size(file)
                    )
                    documents.add(doc)
                    logger.debug { "  ‚úì ${doc.path} (${doc.lines} lines, ${doc.size} bytes)" }
                } catch (e: Exception) {
                    logger.warn { "  ‚úó –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ $file: ${e.message}" }
                }
            }
        }
        
        logger.info { "‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ ${documents.size} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" }
        logger.info { "üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: ${documents.sumOf { it.size } / 1024} KB" }
        
        // –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏
        // –£–º–µ–Ω—å—à–∏–ª–∏ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ —Å 2000 –¥–æ 1000 –¥–ª—è –ª—É—á—à–µ–π –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        chunks.clear()
        documents.forEach { doc ->
            val docChunks = doc.toChunks(maxChunkSize = 1500)
            chunks.addAll(docChunks)
            if (docChunks.size > 1) {
                logger.debug { "  üìÑ ${doc.path}: —Ä–∞–∑–±–∏—Ç –Ω–∞ ${docChunks.size} —á–∞–Ω–∫–æ–≤" }
            }
        }
        logger.info { "üì¶ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: ${chunks.size}" }
        
        // –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
        if (vectorizationEnabled && ollamaClient != null) {
            logger.info { "üî¢ –ù–∞—á–∏–Ω–∞—é –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é ${chunks.size} —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ Ollama..." }
            try {
                documentEmbeddings.addAll(
                    ollamaClient.embedBatch(chunks.map { it.content })
                )
                logger.info { "‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (${documentEmbeddings.size} –≤–µ–∫—Ç–æ—Ä–æ–≤)" }
            } catch (e: Exception) {
                logger.error(e) { "‚ùå –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏" }
                logger.warn { "–ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è keyword search)" }
            }
        } else {
            logger.info { "‚è≠Ô∏è –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (keyword search)" }
        }
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ –∫–µ—à
        saveIndex()
    }
    
    /**
     * –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É
     */
    fun search(query: String, limit: Int = 5): List<Document> {
        logger.debug { "üîç –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '$query'" }
        
        // –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ –∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if (vectorizationEnabled && documentEmbeddings.isNotEmpty() && ollamaClient != null) {
            return searchVector(query, limit)
        }
        
        // Fallback: keyword search
        val results = documents
            .filter { it.matches(query) }
            .sortedByDescending { doc ->
                // –ü—Ä–æ—Å—Ç–æ–π —Å–∫–æ—Ä–∏–Ω–≥: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∑–∞–ø—Ä–æ—Å–∞
                query.split(" ")
                    .filter { it.length > 2 }
                    .sumOf { term -> 
                        doc.content.split(Regex("\\W+"))
                            .count { it.equals(term, ignoreCase = true) }
                    }
            }
            .take(limit)
        
        logger.debug { "  –ù–∞–π–¥–µ–Ω–æ: ${results.size} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (keyword search)" }
        return results
    }
    
    /**
     * –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Ollama (–ø–æ —á–∞–Ω–∫–∞–º)
     */
    private fun searchVector(query: String, limit: Int): List<Document> {
        logger.debug { "üî¢ –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ ${chunks.size} —á–∞–Ω–∫–∞–º..." }
        
        try {
            // –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            val queryEmbedding = ollamaClient!!.embed(query)
            
            // –í—ã—á–∏—Å–ª—è–µ–º similarity –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
            val chunkSimilarities = chunks.mapIndexed { index, chunk ->
                val similarity = if (index < documentEmbeddings.size) {
                    ollamaClient.cosineSimilarity(queryEmbedding, documentEmbeddings[index])
                } else {
                    0.0
                }
                
                Triple(chunk, similarity, index)
            }
            
            // –ë–µ—Ä–µ–º —Ç–æ–ø —á–∞–Ω–∫–æ–≤
            val topChunks = chunkSimilarities
                .sortedByDescending { it.second }
                .take(limit)  // –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤, —Ç.–∫. –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
            logger.debug { "  –¢–æ–ø —á–∞–Ω–∫–æ–≤:" }
            topChunks.take(5).forEach { (chunk, sim, idx) ->
                logger.debug { "    ${chunk.id} (similarity: ${(sim * 100).toInt()}%)" }
            }
            
            // –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —á–∞–Ω–∫–∏ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –∏ —Å–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            val docPaths = topChunks
                .map { it.first.path }
                .distinct()
                .take(limit)
            
            // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            val results = documents.filter { doc -> 
                docPaths.contains(doc.path) 
            }
            
            logger.debug { "  –ù–∞–π–¥–µ–Ω–æ: ${results.size} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ ${topChunks.size} —á–∞–Ω–∫–æ–≤ (vector search)" }
            
            return results
            
        } catch (e: Exception) {
            logger.error(e) { "–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, fallback –Ω–∞ keyword search" }
            return search(query, limit)
        }
    }
    
    /**
     * –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
     */
    data class ContextResult(
        val context: String,
        val sources: List<String>
    )
    
    /**
     * –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
     * –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥: threshold filtering + LLM scoring
     */
    suspend fun rerankSearch(query: String, topK: Int = 3): ContextResult {
        if (reranker == null) {
            logger.warn { "Reranker –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫" }
            return buildContext(query, topK)
        }
        
        if (!vectorizationEnabled || documentEmbeddings.isEmpty() || ollamaClient == null) {
            logger.warn { "–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ –≤–∫–ª—é—á–µ–Ω–∞, —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω" }
            return buildContext(query, topK)
        }
        
        logger.info { "üîÑ –ó–∞–ø—É—Å–∫ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: \"$query\"" }
        
        try {
            // –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            val queryEmbedding = ollamaClient.embed(query)
            
            // –ù–∞—Ö–æ–¥–∏–º –í–°–ï —á–∞–Ω–∫–∏ —Å similarity
            val allChunksWithSim = chunks.mapIndexed { index, chunk ->
                val similarity = if (index < documentEmbeddings.size) {
                    ollamaClient.cosineSimilarity(queryEmbedding, documentEmbeddings[index])
                } else {
                    0.0
                }
                Pair(chunk, similarity)
            }.sortedByDescending { it.second }
            
            logger.info { "–ù–∞–π–¥–µ–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞: ${allChunksWithSim.size}" }
            
            // –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
            val rerankResult = reranker.hybridRerank(
                query = query,
                chunks = allChunksWithSim,
                options = Reranker.RerankOptions(
                    minSimilarity = 0.25,
                    topK = topK,
                    maxChunksForLLM = 20  // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º LLM-–æ—Ü–µ–Ω–∫—É —Ç–æ–ø-20 (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)
                )
            )
            
            if (rerankResult.chunks.isEmpty()) {
                return ContextResult(
                    context = "–ü–æ—Å–ª–µ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.",
                    sources = emptyList()
                )
            }
            
            // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–∞–∂–µ –µ—Å–ª–∏ –æ—Ü–µ–Ω–∫–∞ –Ω–∏–∑–∫–∞—è (> 0)
            val bestChunk = rerankResult.chunks.first()
            logger.info { "üéØ –õ—É—á—à–∏–π —á–∞–Ω–∫: ${bestChunk.llmScore}/10" }
            
            val context = rerankResult.chunks.joinToString("\n\n" + "=".repeat(80) + "\n\n") { rankedChunk ->
                """
                |üìÑ –§–∞–π–ª: ${rankedChunk.chunk.path} (—á–∞–Ω–∫ ${rankedChunk.chunk.chunkIndex})
                |   üéØ LLM –æ—Ü–µ–Ω–∫–∞: ${rankedChunk.llmScore.toInt()}/10, Similarity: ${(rankedChunk.similarity * 100).toInt()}%
                |
                |${rankedChunk.chunk.content}
                """.trimMargin()
            }
            
            val sources = rerankResult.chunks.map { it.chunk.path }.distinct()
            
            logger.info { "‚úÖ –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: ${rerankResult.chunks.size} —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤" }
            
            return ContextResult(
                context = context,
                sources = sources
            )
            
        } catch (e: Exception) {
            logger.error(e) { "–û—à–∏–±–∫–∞ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞, fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫" }
            return buildContext(query, topK)
        }
    }
    
    /**
     * –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤/—á–∞–Ω–∫–æ–≤
     */
    fun buildContext(query: String, maxDocs: Int = 3): ContextResult {
        // –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —á–∞–Ω–∫–∏ —Å vector search
        if (vectorizationEnabled && documentEmbeddings.isNotEmpty() && ollamaClient != null) {
            return buildContextFromChunks(query, maxChunks = maxDocs * 2)
        }
        
        // Fallback: keyword search –ø–æ —á–∞–Ω–∫–∞–º (–Ω–µ –ø–æ —Ü–µ–ª—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º)
        return buildContextFromChunksKeyword(query, maxChunks = maxDocs * 2)
    }
    
    /**
     * Keyword search –ø–æ —á–∞–Ω–∫–∞–º (fallback –∫–æ–≥–¥–∞ –Ω–µ—Ç –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏)
     */
    private fun buildContextFromChunksKeyword(query: String, maxChunks: Int = 4): ContextResult {
        val queryTerms = query.lowercase()
            .split(Regex("[\\s,.?!]+"))
            .filter { it.length > 2 }
        
        if (queryTerms.isEmpty() || chunks.isEmpty()) {
            return ContextResult(
                context = "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.",
                sources = emptyList()
            )
        }
        
        // –°–∫–æ—Ä–∏–Ω–≥ —á–∞–Ω–∫–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö —Ç–µ—Ä–º–æ–≤
        val scoredChunks = chunks.map { chunk ->
            val score = queryTerms.sumOf { term ->
                chunk.content.lowercase().split(Regex("\\W+"))
                    .count { it == term }
            }
            Pair(chunk, score)
        }
            .filter { it.second > 0 }
            .sortedByDescending { it.second }
            .take(maxChunks)
        
        if (scoredChunks.isEmpty()) {
            return ContextResult(
                context = "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.",
                sources = emptyList()
            )
        }
        
        logger.debug { "  –ù–∞–π–¥–µ–Ω–æ ${scoredChunks.size} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (keyword search)" }
        
        val context = scoredChunks.joinToString("\n\n" + "=".repeat(80) + "\n\n") { (chunk, score) ->
            """
            |üìÑ –§–∞–π–ª: ${chunk.path} (—á–∞–Ω–∫ ${chunk.chunkIndex}, —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: $score)
            |
            |${chunk.content}
            """.trimMargin()
        }
        
        return ContextResult(
            context = context,
            sources = scoredChunks.map { it.first.path }.distinct()
        )
    }
    
    /**
     * –§–æ—Ä–º–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
     */
    private fun buildContextFromChunks(query: String, maxChunks: Int = 3): ContextResult {  // –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 2
        try {
            val queryEmbedding = ollamaClient!!.embed(query)
            
            // –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø —á–∞–Ω–∫–æ–≤ (–±–µ—Ä—ë–º –ú–ï–ù–¨–®–ï –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)
            val topChunks = chunks.mapIndexed { index, chunk ->
                val similarity = if (index < documentEmbeddings.size) {
                    ollamaClient.cosineSimilarity(queryEmbedding, documentEmbeddings[index])
                } else {
                    0.0
                }
                Pair(chunk, similarity)
            }
                .sortedByDescending { it.second }
                .take(maxChunks)
            
            if (topChunks.isEmpty()) {
                return ContextResult(
                    context = "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.",
                    sources = emptyList()
                )
            }
            
            logger.debug { "  –ò—Å–ø–æ–ª—å–∑—É–µ–º ${topChunks.size} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞" }
            topChunks.forEachIndexed { i, (chunk, sim) ->
                logger.debug { "    ${i+1}. ${chunk.path} (—á–∞–Ω–∫ ${chunk.chunkIndex}, similarity: ${(sim * 100).toInt()}%)" }
            }
            
            val context = topChunks.joinToString("\n\n" + "=".repeat(80) + "\n\n") { (chunk, similarity) ->
                """
                |üìÑ –§–∞–π–ª: ${chunk.path} (—á–∞–Ω–∫ ${chunk.chunkIndex}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: ${(similarity * 100).toInt()}%)
                |
                |${chunk.content}
                """.trimMargin()
            }
            
            // –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ñ–∞–π–ª—ã)
            val sources = topChunks.map { it.first.path }.distinct()
            
            return ContextResult(
                context = context,
                sources = sources
            )
            
        } catch (e: Exception) {
            logger.error(e) { "–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —á–∞–Ω–∫–æ–≤" }
            return ContextResult(
                context = "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É.",
                sources = emptyList()
            )
        }
    }
    
    /**
     * –ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã –ø–æ glob-–ø–∞—Ç—Ç–µ—Ä–Ω—É
     */
    private fun findFilesByPattern(pattern: String): List<Path> {
        // –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä—è–º–æ–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        val directPath = projectPath.resolve(pattern)
        if (directPath.isRegularFile()) {
            return listOf(directPath)
        }
        
        // –ï—Å–ª–∏ —ç—Ç–æ –ø–∞—Ç—Ç–µ—Ä–Ω —Ç–∏–ø–∞ "docs/*.md"
        if (pattern.contains("*")) {
            val parts = pattern.split("/")
            val dir = parts.dropLast(1).joinToString("/")
            val filePattern = parts.last()
            
            val searchDir = if (dir.isEmpty()) projectPath else projectPath.resolve(dir)
            
            if (!Files.exists(searchDir)) {
                logger.warn { "–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: $searchDir" }
                return emptyList()
            }
            
            return Files.walk(searchDir, if (pattern.contains("**")) Int.MAX_VALUE else 1)
                .filter { it.isRegularFile() }
                .filter { matchesPattern(it.fileName.toString(), filePattern) }
                .filter { !shouldIgnore(it) }
                .toList()
        }
        
        // –ü—Ä–æ—Å—Ç–æ–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        val simplePath = projectPath.resolve(pattern)
        return if (Files.exists(simplePath) && simplePath.isRegularFile()) {
            listOf(simplePath)
        } else {
            emptyList()
        }
    }
    
    /**
     * –ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—É
     */
    private fun matchesPattern(fileName: String, pattern: String): Boolean {
        if (pattern == "*") return true
        if (pattern == "*.md") return fileName.endsWith(".md")
        if (pattern.startsWith("*.")) {
            val ext = pattern.substring(1)
            return fileName.endsWith(ext)
        }
        return fileName == pattern
    }
    
    /**
     * –ü—Ä–æ–≤–µ—Ä–∫–∞, –Ω—É–∂–Ω–æ –ª–∏ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª
     */
    private fun shouldIgnore(path: Path): Boolean {
        val pathStr = path.toString()
        return config.project.ignore.any { ignore ->
            pathStr.contains(ignore)
        }
    }
}

